# Deep Dive into Transformers, Neural Network & Backprop

Dive deep into Transformers, Neural Networks, and backpropagation. Explore cutting-edge AI, uncover insights, and empower your understanding of these transformative technologies.

<br/>

import Image from "next/image";

<div style={{ position: "relative" }}>
  <Image
    src="/static/img/memes/backpropAndTransformersMeme.png"
    alt="backprop & Transformers Meme"
    height="300"
    width="850"
    priority
  />
</div>

### Courses

- [Neural Network course](https://www.youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi) by 3Blue1Brown, Delve into the intricate world of neural networks through captivating animations and intuitive explanations. Gain a deep understanding of this fundamental ML concept in a visually engaging and enlightening way.

- [Neural Networks / Deep Learning](https://www.youtube.com/playlist?list=PLblh5JKOoLUIxGDQs4LFFD--41Vzf-ME1) by StatQuest with Josh Starmer, includes everything you need to know about Neural Networks, from the basics, all the way to image classification with Convolutional Neural Networks.

- [Hugging Face Transformers](https://huggingface.co/learn/nlp-course/chapter1/1): covers core concepts of the Transformers library, model operation, fine-tuning from Hugging Face Hub, and result sharing. It then proceeds to Datasets and Tokenizers for NLP tasks. The final section delves into speech processing and computer vision tasks, emphasizing model optimization and production-ready demos.

### Articles

- [Introduction to Neural Networks â€” Part 1](https://medium.com/deep-learning-demystified/introduction-to-neural-networks-part-1-e13f132c6d7e) & [part 2](https://medium.com/deep-learning-demystified/introduction-to-neural-networks-part-2-c261a99f4138) by Harsha Bommana, explores the neural network's fundamental components, including neurons, their mathematical operations, and the significance of activation functions for addressing non-linear problems. It delves into various neural network types and offers examples of their applications in ML and deep learning.

- [How Transformers Work](https://towardsdatascience.com/transformers-141e32e69591): Discover the mechanics of Transformers, the revolutionary neural network technology harnessed by industry leaders like OpenAI and DeepMind. Gain valuable insights into the inner workings of these AI giants and explore their transformative applications in the world of ML.

- [The illustrated transformer](https://jalammar.github.io/illustrated-transformer/) by Jay Alammar provides an in-depth technical examination of the transformer framework, offering detailed insights into its structure and functionality.

- [Yes you should understand backprop](https://karpathy.medium.com/yes-you-should-understand-backprop-e2f06eab496b) by Andrej Karpathy, depth post on backpropagation provides valuable insights into the intricacies of this crucial technique.

- [RLHF: Reinforcement Learning from Human Feedback](https://huyenchip.com/2023/05/02/rlhf.html): Explore Chip Huyen's insights into RLHF, a transformative approach that enhances the predictability and human-friendliness of LLMs. Gain valuable knowledge about this crucial aspect of systems like ChatGPT, shedding light on its significance and potential impact.

- [The Annotated Transformer](http://nlp.seas.harvard.edu/annotated-transformer/): This post provides an annotated paper implementation of ["Attention Is All You Need"](https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf), presented in a line-by-line format. It reorganizes and omits certain sections from the original paper while incorporating comments, offering a fully functional implementation with available code in a concise format. Requires some knowledge of PyTorch.

### Explainers

- Explore the video article [Attention Is All You Need video](https://www.youtube.com/watch?v=XowwKOAWYoQ) a visual journey through the groundbreaking concepts introduced in the original paper. If the written article seems complex, this video provides an accessible and insightful way to grasp the key ideas behind the transformative Transformer architecture.

- [Introduction to Transformers](https://www.youtube.com/watch?v=XfpMkf4rD6E&t=2287s) by Andrej Karpathy. Since their groundbreaking introduction in 2017, transformers have transformed NLP and expanded into various domains of Deep Learning, including computer vision (CV), reinforcement learning (RL), Generative Adversarial Networks (GANs), Speech, and even Biology. They played a pivotal role in the development of powerful language models like GPT-3 and were instrumental in DeepMind's remarkable AlphaFold2 project, addressing protein folding challenges.

- [A friendly introduction to Deep Learning and Neural Networks](https://www.youtube.com/watch?v=BR9h47Jtqyw&list=PLs8w1Cdi-zvavXlPXEAsWIh4Cgh83pZPO&index=1&t=1s) by Serrano.Academy, introduces deep learning and neural networks, explaining AI, ML, and deep learning basics. It showcases real-world applications like image recognition and natural language processing.

- [Watching Neural Networks Learn](https://www.youtube.com/watch?v=TkwXa7Cvfr8&t=1191s) by Emergent Garden, demonstrates neural network learning, focusing on recognizing handwritten digits. Trained on a dataset, it showcases the network's improving accuracy, revealing its capacity for image recognition and evolving internal representations over time.

- [Why Neural Networks can learn (almost) anything](https://www.youtube.com/watch?v=0QczhVg5HaI) by Emergent Garden, provides a fascinating insight into the power of neural networks and their ability to learn complex patterns.

- [The backpropagation algorithm](https://www.youtube.com/watch?v=VCT1N0EsGj0) by Geoffrey Hinton delves into the fundamentals of backpropagation, a key algorithm in training neural networks. Hinton's teachings include insights on learning with the idea behind it, the role of hidden units, and learning through perturbing weights, discusses the concept of learning by using perturbations, providing valuable knowledge and techniques for training neural networks effectively.

- [Tensors for Neural Networks, Clearly Explained!!!](https://www.youtube.com/watch?v=L35fFDpwIM4) by StatQuest with Josh Starmer. Tensors are fundamental data structures in machine learning, representing multi-dimensional arrays that store and manipulate information, enabling the foundation for deep learning and neural network operations.

### Papers

- [Attention Is All You Need](https://arxiv.org/abs/1706.03762) (2017): Introduced the groundbreaking concept of the Transformer architecture, an attention-based neural architecture for sequence processing. It outperforms other methods in NLP tasks like machine translation and language modeling, demonstrating its effectiveness in capturing contextual information from input sequences like sentences. [(blog)](https://blog.research.google/2017/08/transformer-novel-neural-network.html)

- [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805) (2018): Presents BERT, a pre-trained deep bidirectional transformer model. Trained on extensive text data with a masked language modeling objective, it excels in natural NLP like question answering and sentiment analysis, surpassing alternative methods in performance and versatility.

- [Improving Language Understanding by Generative Pre-Training](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf) (2018): Introduced Generative Pre-trained Transformer (GPT), enhancing natural language understanding through unsupervised learning, addressing the scarcity of labeled data. It introduces "generative pre-training," training a large neural network on vast unlabeled text corpora, followed by fine-tuning on specific tasks with labeled data. The approach combines transformers and unsupervised pre-training, showing improved language model performance and task-related advancements. [(blog)](https://openai.com/research/language-unsupervised) [(code)](https://github.com/openai/finetune-transformer-lm)

- [Language Models are Unsupervised Multitask Learners](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) (2019): Introduced GPT-2, enhancing natural language processing through unsupervised learning, showcasing a language model's ability to learn tasks without explicit supervision. Training on the WebText dataset, the model infers and performs tasks in natural language sequences, demonstrating superior performance in a zero-shot setting across various tasks compared to discriminatively trained models. [(blog)](https://openai.com/research/better-language-models) [(code)](https://github.com/openai/gpt-2)

- [Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context](https://arxiv.org/abs/1901.02860) (2019): Suggests improving language models with Transformer-XL, designed for longer text. Uses segment-level recurrence, reusing hidden states for processing extended sequences. Outperforms original Transformer and other models in language tasks.

- [Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer](https://arxiv.org/abs/1910.10683) (2019)

- [ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators](https://arxiv.org/abs/2003.10555) (2020)

- [Language Models are Few-Shot Learners](https://arxiv.org/abs/2005.14165) (2020)

- [Rethinking Attention with Performers](https://arxiv.org/abs/2009.14794) (2020)

- [End-to-End Object Detection with Transformers](https://arxiv.org/abs/2005.12872) (2020)

- [Pretraining Data Mixtures Enable Narrow Model Selection Capabilities in Transformer Models](https://arxiv.org/abs/2311.00871) (2023): The study reveals that transformer models, pre-trained on a mix of diverse data sources like news, books, and code, exhibit constrained 'model selection' abilities. They excel in tasks aligned with their training domains but struggle in mismatched ones. While pretraining on varied data holds potential for enhancing model flexibility, further research is required to broaden transformers' task generalization.
